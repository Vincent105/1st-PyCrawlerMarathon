{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ettoday 網路爬蟲實作練習\n",
    "\n",
    "\n",
    "* 能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ettoday 網頁爬蟲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先複習一下原本純靜態的爬法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先複習一下原本純靜態的爬法\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ettoday.net/news/news-list.htm'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html5lib\")\n",
    "\n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從上面的結果來看，你會發現它只會抓到最近的資料。原因是因為資料是透過下滑的過程中，利用 JavaScript 動態載入的。因此，這邊我們必須利用 selenium 這樣的工具來輔助："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='chromedriver\\chromedriver.exe')\n",
    "\n",
    "browser.get(\"https://www.ettoday.net/news/news-list.htm\")# 每個兩秒鐘自動往下滑\n",
    "\n",
    "for i in range(10):\n",
    "    time.sleep(2)\n",
    "    browser.execute_script(\"window.scrollBy(0,10000)\")\n",
    "\n",
    "# 取得資料，丟到 BeautifulSoup 解析\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, \"html5lib\")\n",
    "\n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關閉瀏覽器\n",
    "browser.quit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 取出今天所有的新聞\n",
    "* 取出今天下午三點到五點的新聞\n",
    "* 根據範例，取出三天前下午三點到五點的新聞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得資料，丟到 BeautifulSoup 解析\n",
    "\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, \"html5lib\")\n",
    "\n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}